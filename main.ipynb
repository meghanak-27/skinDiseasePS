{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class driveFile:\n",
    "  def __init__(self):\n",
    "    self.path=\"\"\n",
    "  def dataload(self,folder_path):\n",
    "    img=[]\n",
    "    label=[]\n",
    "    import os\n",
    "    self.path1=os.path.join(self.path,folder_path)\n",
    "    \n",
    "    if os.path.exists(self.path1):\n",
    "      for i in os.listdir(self.path1):\n",
    "        self.dis_path=os.path.join(self.path1,i)\n",
    "        # /content/drive/MyDrive/train_set/BA- cellulitis\n",
    "        if os.path.exists(self.dis_path) and '.DS_Store' not in self.dis_path:\n",
    "          for j in os.listdir(self.dis_path):\n",
    "           if j.endswith(('.jpg', '.png', '.jpeg')):\n",
    "              self.img_path = os.path.join(self.dis_path, j)\n",
    "              loaded_img= self.imgload(self.img_path)\n",
    "              if loaded_img is not None:\n",
    "                img.append(loaded_img)\n",
    "                label.append(i)\n",
    "              else:\n",
    "                 print(\"Skipping non-image file:\", j)\n",
    "          else:\n",
    "              print(\"Skipping invalid path or .DS_Store:\", self.dis_path)\n",
    "              continue\n",
    "      return np.array(img), np.array(label)\n",
    "    else:\n",
    "      return \"patherror\",\"patherror\"\n",
    "\n",
    "\n",
    "\n",
    "  def imgload(self,imgpath):\n",
    "    img=cv2.imread(imgpath)\n",
    "    if img is None:\n",
    "      print(f\"Failed to load image at {imgpath}\")\n",
    "      return None\n",
    "    img=cv2.resize(img,(20,20))\n",
    "    # v imp, usually 224*224 but here 20*20 \"downsampling or using a smaller image resolution before flattening it.\"\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    img=img.astype('float32')/255.0\n",
    "    \n",
    "    img_array=np.array(img)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid path or .DS_Store: dataset/train_set/FU-athlete-foot\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/VI-chickenpox\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/VI-shingles\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/FU-nail-fungus\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/BA-impetigo\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/PA-cutaneous-larva-migrans\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/FU-ringworm\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/BA- cellulitis\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/FU-athlete-foot\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/VI-chickenpox\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/VI-shingles\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/FU-nail-fungus\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/BA-impetigo\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/PA-cutaneous-larva-migrans\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/FU-ringworm\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/BA- cellulitis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drive=driveFile()\n",
    "images,labels=drive.dataload('dataset/train_set')\n",
    "test_images,test_labels=drive.dataload('dataset/test_set')\n",
    "images = (images - np.mean(images)) / np.std(images)\n",
    "test_images = (test_images - np.mean(test_images)) / np.std(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=np.array([img.flatten() for img in images])\n",
    "test_images=np.array([img.flatten() for img in test_images])\n",
    "labels=np.array(labels)\n",
    "test_labels=np.array(test_labels)\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "integer_encoded = np.array([label_to_index[label] for label in labels])\n",
    "test_integer_encoded = np.array([label_to_index[label] for label in test_labels])\n",
    "one_hot_encoded = np.eye(len(unique_labels))[integer_encoded]\n",
    "test_one_hot_encoded = np.eye(len(unique_labels))[test_integer_encoded]\n",
    "labels=one_hot_encoded\n",
    "test_labels=test_one_hot_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input=None\n",
    "        self.output=None\n",
    "    def forward(self,input):\n",
    "        # to be overridden \n",
    "        pass \n",
    "    def backward(self,output_gradient,learning_rate):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        self.weights=np.random.randn(input_size,output_size)*0.01\n",
    "        # self.weights = np.random.randn(input_size, output_size) * np.sqrt(2 / (input_size + output_size))\n",
    "\n",
    "        self.bias=np.random.randn(1,output_size)\n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        return np.dot(self.input,self.weights)+self.bias\n",
    "    def backward(self,output_gradient,learning_rate):\n",
    "        w_gradient = np.dot( self.input.T,output_gradient)  # (output_size, input_size)\n",
    "        b_gradient = np.sum(output_gradient, axis=0, keepdims=True)  # (output_size, 1).T if axis=0 else axis=1 output_size, 1\n",
    "        input_gradient = np.dot(output_gradient,self.weights.T)  # (input_size, batch_size)\n",
    "        self.weights-=learning_rate * w_gradient\n",
    "        self.bias-=learning_rate * b_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self,activation,activatn_derivative):\n",
    "        #activation is a variable pointing to a activation method,for ex sigmoid\n",
    "        self.activation=activation\n",
    "        self.activatn_derivative=activatn_derivative\n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        return self.activation(self.input)\n",
    "    def backward(self,output_gradient,learning_rate):\n",
    "        return self.activatn_derivative(self.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1/(1+np.exp(-x))\n",
    "        def sigmoid_derivative(x):\n",
    "            s=sigmoid(x)\n",
    "            return s*(1-s)\n",
    "        super().__init__(sigmoid,sigmoid_derivative)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        def tanh_derivative(x):\n",
    "            t=tanh(x)\n",
    "            return 1-(t**2)\n",
    "        super().__init__(tanh,tanh_derivative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        self.weights=np.random.randn(input_size,output_size)*0.01\n",
    "        self.bias=np.random.randn(1,output_size)\n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        sumation=np.dot(self.input,self.weights)+self.bias\n",
    "        self.softmax=np.exp(sumation)/np.sum(np.exp(sumation),axis=1, keepdims=True)\n",
    "        return self.softmax\n",
    "    def backward(self,output_gradient,learning_rate):\n",
    "        w_gradient = np.dot(self.input.T,output_gradient)/self.input.shape[0]  # (output_size, input_size)\n",
    "        b_gradient = np.sum(output_gradient, axis=0, keepdims=True)  # (output_size, 1)\n",
    "        input_gradient = np.dot(output_gradient,self.weights.T)  # (input_size, batch_size)\n",
    "        self.weights-=learning_rate * w_gradient\n",
    "        self.bias-=learning_rate * b_gradient\n",
    "        return input_gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(Layer):\n",
    "    def __init__(self,loss_fn,loss_fn_Derivative):\n",
    "        self.loss_fn=loss_fn\n",
    "        self.loss_fn_Derivative=loss_fn_Derivative\n",
    "    def forward(self,y_pred,y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return self.loss_fn(y_pred, y_true)\n",
    "    def backward(self,y_pred,y_true):\n",
    "        return self.loss_fn_Derivative(y_pred,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy(Loss):\n",
    "    def __init__(self):\n",
    "        def bce(y_pred,y_true):\n",
    "            y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)  # Prevent extreme values\n",
    "            return -np.mean((y_true*np.log(y_pred))+((1-y_true)*np.log(1-y_pred)))\n",
    "        def bce_derivative(y_pred,y_true):\n",
    "            y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)  # Avoid division by zero\n",
    "            return -(y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "        super().__init__(bce,bce_derivative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCategoricalCE(Loss):\n",
    "    def __init__(self):\n",
    "        def sce(logits,true_labels):\n",
    "            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True)) \n",
    "            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "            true_class_probs = probabilities[np.arange(len(true_labels)), true_labels]\n",
    "            log_loss = -np.log(true_class_probs + 1e-15)  # Adding a small constant for numerical stability\n",
    "            return np.mean(log_loss)\n",
    "        def sce_derivative(logits, true_labels):\n",
    "            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Stability trick\n",
    "            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "            batch_size = logits.shape[0]\n",
    "            probabilities[np.arange(batch_size), true_labels] -= 1\n",
    "            gradient = probabilities / batch_size\n",
    "            \n",
    "            return gradient\n",
    "        super().__init__(sce,sce_derivative)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CategoricalCrossEntropyLoss(Loss):\n",
    "    def __init__(self):\n",
    "        def cel(y_predSM,y_true):\n",
    "            # direct correspondance of values, element wise multi\n",
    "            return -np.mean(np.sum(y_true * np.log(y_predSM), axis=1))\n",
    "        def cel_derivative(y_predSM,y_true):\n",
    "            return y_predSM-y_true\n",
    "        super().__init__(cel,cel_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model:\n",
    "    def train(self,images,labels,layers,hyperpara):\n",
    "        \n",
    "        for key, value in hyperpara.items():\n",
    "            setattr(self, key, value)\n",
    "        self.loss_fn=CategoricalCrossEntropyLoss()\n",
    "        self.X=images\n",
    "        self.y=labels\n",
    "        self.layers=layers\n",
    "        self.softmax=Softmax(self.neurons[-1],self.softmax_neurons)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            num_samples = self.X.shape[0]\n",
    "            index=np.arange(num_samples)\n",
    "            np.random.shuffle(index)\n",
    "            self.X=self.X[index]\n",
    "            self.y=self.y[index]\n",
    "            epoch_loss=0\n",
    "            for i in range(0, num_samples, self.batch_size):\n",
    "                if(i+self.batch_size==num_samples):\n",
    "                    X_batch = self.X[i:num_samples]\n",
    "                    y_batch = self.y[i:num_samples]\n",
    "                else:\n",
    "                    X_batch = self.X[i:i + self.batch_size]\n",
    "                    y_batch = self.y[i:i + self.batch_size]\n",
    "                \n",
    "                ActiPredicted=self.forward(X_batch)\n",
    "                \n",
    "                pred_y=self.softmax.forward(ActiPredicted)\n",
    "                \n",
    "                loss=self.loss_fn.forward(pred_y,y_batch)\n",
    "                epoch_loss += loss\n",
    "                loss_gradient=self.loss_fn.backward(pred_y,y_batch)\n",
    "                # print(f\"Epoch {epoch + 1}, Batch {i // self.batch_size + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "                # Backward pass\n",
    "                \n",
    "                self.backward(loss_gradient)\n",
    "            print(f\"Epoch {epoch + 1}, Average Loss: {epoch_loss / (num_samples / self.batch_size):.4f}\")\n",
    "                \n",
    "                \n",
    "                    \n",
    "                \n",
    "    def forward(self,X):\n",
    "        for layer in self.layers:\n",
    "            # here layer is obj to class not class*\n",
    "            X=layer.forward(X)\n",
    "        return X\n",
    "        \n",
    "\n",
    "    def backward(self, loss_gradient):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_gradient = layer.backward(loss_gradient, self.learning_rate)\n",
    "    def predict(self, X):\n",
    "        logits = self.forward(X)\n",
    "        probabilities = self.softmax.forward(logits)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        true_labels = np.argmax(y, axis=1)\n",
    "        accuracy = np.mean(predictions == true_labels)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return accuracy\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 2.4946\n",
      "Epoch 2, Average Loss: 2.4933\n",
      "Epoch 3, Average Loss: 2.4934\n",
      "Epoch 4, Average Loss: 2.4933\n",
      "Epoch 5, Average Loss: 2.4962\n",
      "Epoch 6, Average Loss: 2.4950\n",
      "Epoch 7, Average Loss: 2.4936\n",
      "Epoch 8, Average Loss: 2.4952\n",
      "Epoch 9, Average Loss: 2.4946\n",
      "Epoch 10, Average Loss: 2.4942\n"
     ]
    }
   ],
   "source": [
    "hyperparameters={\n",
    "    \"hiddenLayers\":6,\n",
    "    \"learning_rate\":0.05,\n",
    "    \"batch_size\":16,\n",
    "    \"epochs\":10,\n",
    "    \"ActivationFns\":None,\n",
    "    \"neurons\":None,\n",
    "    \"softmax_neurons\":labels.shape[1]\n",
    "}\n",
    "# change\n",
    "hyperparameters[\"neurons\"]=[64,64,32,32,16,16]\n",
    "hyperparameters[\"ActivationFns\"]=[Sigmoid(),Sigmoid(),Sigmoid(),Tanh(),Tanh(),Tanh()]\n",
    "layers=[]\n",
    "inputsize=images.shape[1]\n",
    "for i in range(hyperparameters[\"hiddenLayers\"]):\n",
    "    outputsize=hyperparameters[\"neurons\"][i]\n",
    "    layers.extend([Dense(inputsize,outputsize),hyperparameters[\"ActivationFns\"][i]])\n",
    "    inputsize=outputsize\n",
    "\n",
    "\n",
    "model=Model()\n",
    "model.train(images,labels,layers,hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.73%\n",
      "accuary:0.1072961373390558\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuary:{model.evaluate(test_images,test_labels)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skindisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
