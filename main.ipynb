{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class driveFile:\n",
    "  def __init__(self):\n",
    "    self.path=\"\"\n",
    "  def dataload(self,folder_path):\n",
    "    img=[]\n",
    "    label=[]\n",
    "    import os\n",
    "    self.path1=os.path.join(self.path,folder_path)\n",
    "    \n",
    "    if os.path.exists(self.path1):\n",
    "      for i in os.listdir(self.path1):\n",
    "        self.dis_path=os.path.join(self.path1,i)\n",
    "        # /content/drive/MyDrive/train_set/BA- cellulitis\n",
    "        if os.path.exists(self.dis_path) and '.DS_Store' not in self.dis_path:\n",
    "          for j in os.listdir(self.dis_path):\n",
    "           if j.endswith(('.jpg', '.png', '.jpeg')):\n",
    "              self.img_path = os.path.join(self.dis_path, j)\n",
    "              loaded_img= self.imgload(self.img_path)\n",
    "              if loaded_img is not None:\n",
    "                img.append(loaded_img)\n",
    "                label.append(i)\n",
    "              else:\n",
    "                 print(\"Skipping non-image file:\", j)\n",
    "          else:\n",
    "              print(\"Skipping invalid path or .DS_Store:\", self.dis_path)\n",
    "              continue\n",
    "      return np.array(img), np.array(label)\n",
    "    else:\n",
    "      return \"patherror\",\"patherror\"\n",
    "\n",
    "\n",
    "\n",
    "  def imgload(self,imgpath):\n",
    "    img=cv2.imread(imgpath)\n",
    "    if img is None:\n",
    "      print(f\"Failed to load image at {imgpath}\")\n",
    "      return None\n",
    "    img=cv2.resize(img,(20,20))\n",
    "    # v imp, usually 224*224 but here 20*20 \"downsampling or using a smaller image resolution before flattening it.\"\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    img=img.astype('float32')/255.0\n",
    "    \n",
    "    img_array=np.array(img)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid path or .DS_Store: dataset/train_set/FU-athlete-foot\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/VI-chickenpox\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/VI-shingles\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/FU-nail-fungus\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/BA-impetigo\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/PA-cutaneous-larva-migrans\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/FU-ringworm\n",
      "Skipping invalid path or .DS_Store: dataset/train_set/BA- cellulitis\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/FU-athlete-foot\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/VI-chickenpox\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/VI-shingles\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/FU-nail-fungus\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/BA-impetigo\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/PA-cutaneous-larva-migrans\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/FU-ringworm\n",
      "Skipping invalid path or .DS_Store: dataset/test_set/BA- cellulitis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drive=driveFile()\n",
    "images,labels=drive.dataload('dataset/train_set')\n",
    "test_images,test_labels=drive.dataload('dataset/test_set')\n",
    "images = (images - np.mean(images)) / np.std(images)\n",
    "test_images = (test_images - np.mean(test_images)) / np.std(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=np.array([img.flatten() for img in images])\n",
    "test_images=np.array([img.flatten() for img in test_images])\n",
    "labels=np.array(labels)\n",
    "test_labels=np.array(test_labels)\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "integer_encoded = np.array([label_to_index[label] for label in labels])\n",
    "test_integer_encoded = np.array([label_to_index[label] for label in test_labels])\n",
    "one_hot_encoded = np.eye(len(unique_labels))[integer_encoded]\n",
    "test_one_hot_encoded = np.eye(len(unique_labels))[test_integer_encoded]\n",
    "labels=one_hot_encoded\n",
    "test_labels=test_one_hot_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 67.81%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.48      0.54        33\n",
      "           1       0.59      0.85      0.69        20\n",
      "           2       0.83      0.75      0.79        32\n",
      "           3       0.47      0.24      0.32        33\n",
      "           4       0.50      0.78      0.61        23\n",
      "           5       0.68      0.68      0.68        25\n",
      "           6       0.84      0.91      0.87        34\n",
      "           7       0.79      0.82      0.81        33\n",
      "\n",
      "    accuracy                           0.68       233\n",
      "   macro avg       0.66      0.69      0.66       233\n",
      "weighted avg       0.67      0.68      0.66       233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# benchmarking/sanity checking\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "labels_int = np.argmax(labels, axis=1)\n",
    "test_labels_int = np.argmax(test_labels, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "images_scaled = scaler.fit_transform(images)\n",
    "test_images_scaled = scaler.transform(test_images)\n",
    "\n",
    "# Initialize and train SVM\n",
    "svm_model = SVC(kernel='linear', random_state=42)  # Try 'rbf' kernel too for non-linear data\n",
    "svm_model.fit(images_scaled, labels_int)\n",
    "\n",
    "# Predictions and evaluation\n",
    "svm_predictions = svm_model.predict(test_images_scaled)\n",
    "svm_accuracy = accuracy_score(test_labels_int, svm_predictions)\n",
    "\n",
    "print(f\"SVM Accuracy: {svm_accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels_int, svm_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input=None\n",
    "        self.output=None\n",
    "    def forward(self,input):\n",
    "        # to be overridden \n",
    "        pass \n",
    "    def backward(self,output_gradient,learning_rate):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.velocities_W=np.zeros_like(self.weights)\n",
    "        self.velocities_B=np.zeros_like(self.bias)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        return np.dot(self.input,self.weights)+self.bias\n",
    "    \n",
    "    def backward(self,output_gradient,learning_rate,momentcoeff):\n",
    "        w_gradient = np.dot( self.input.T,output_gradient)  # (output_size, input_size)\n",
    "        b_gradient = np.sum(output_gradient, axis=0, keepdims=True)  # (output_size, 1).T if axis=0 else axis=1 output_size, 1\n",
    "        input_gradient = np.dot(output_gradient,self.weights.T)# (input_size, batch_size)\n",
    "        self.velocities_W=  momentcoeff*self.velocities_W+(1- momentcoeff)* w_gradient\n",
    "        self.velocities_B=  momentcoeff*self.velocities_B +(1- momentcoeff)* b_gradient\n",
    "        self.weights-=(learning_rate*self.velocities_W)\n",
    "        self.bias-=(learning_rate*self.velocities_B)\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self,activation,activatn_derivative):\n",
    "        #activation is a variable pointing to a activation method,for ex sigmoid\n",
    "        self.activation=activation\n",
    "        self.activatn_derivative=activatn_derivative\n",
    "        \n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        return self.activation(self.input)\n",
    "    \n",
    "    def backward(self,output_gradient,learning_rate,momentcoeff):\n",
    "        return self.activatn_derivative(self.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1/(1+np.exp(-x))\n",
    "        def sigmoid_derivative(x):\n",
    "            s=sigmoid(x)\n",
    "            return s*(1-s)\n",
    "        super().__init__(sigmoid,sigmoid_derivative)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        def tanh_derivative(x):\n",
    "            t=tanh(x)\n",
    "            return 1-(t**2)\n",
    "        super().__init__(tanh,tanh_derivative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Activation):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        def prelu(x):\n",
    "            return np.where(x > 0, x, self.alpha * x)\n",
    "        \n",
    "        def prelu_derivative(x):\n",
    "            return np.where(x > 0, 1, self.alpha)\n",
    "        \n",
    "        super().__init__(prelu, prelu_derivative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.velocities_W = np.zeros_like(self.weights)\n",
    "        self.velocities_B = np.zeros_like(self.bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        # Compute logits\n",
    "        logits = np.dot(self.input, self.weights) + self.bias\n",
    "        # Numerically stable softmax\n",
    "        logits_shifted = logits - np.max(logits, axis=1, keepdims=True)\n",
    "        exp_values = np.exp(logits_shifted)\n",
    "        self.softmax = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.softmax\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate, momentcoeff):\n",
    "        # Compute gradients\n",
    "        w_gradient = np.dot(self.input.T, output_gradient) / self.input.shape[0]\n",
    "        b_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "\n",
    "        # Update weights and biases using momentum\n",
    "        self.velocities_W=  momentcoeff*self.velocities_W+(1- momentcoeff)* w_gradient\n",
    "        self.velocities_B=  momentcoeff*self.velocities_B +(1- momentcoeff)* b_gradient\n",
    "        self.weights-=(learning_rate*self.velocities_W)\n",
    "        self.bias-=(learning_rate*self.velocities_B)\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(Layer):\n",
    "    def __init__(self,loss_fn,loss_fn_Derivative):\n",
    "        self.loss_fn=loss_fn\n",
    "        self.loss_fn_Derivative=loss_fn_Derivative\n",
    "    def forward(self,y_pred,y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return self.loss_fn(y_pred, y_true)\n",
    "    def backward(self,y_pred,y_true):\n",
    "        return self.loss_fn_Derivative(y_pred,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy(Loss):\n",
    "    def __init__(self):\n",
    "        def bce(y_pred,y_true):\n",
    "            y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)  # Prevent extreme values\n",
    "            return -np.mean((y_true*np.log(y_pred))+((1-y_true)*np.log(1-y_pred)))\n",
    "        def bce_derivative(y_pred,y_true):\n",
    "            y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)  # Avoid division by zero\n",
    "            return -(y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "        super().__init__(bce,bce_derivative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCategoricalCE(Loss):\n",
    "    def __init__(self):\n",
    "        def sce(logits,true_labels):\n",
    "            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True)) \n",
    "            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "            true_class_probs = probabilities[np.arange(len(true_labels)), true_labels]\n",
    "            log_loss = -np.log(true_class_probs + 1e-15)  # Adding a small constant for numerical stability\n",
    "            return np.mean(log_loss)\n",
    "        def sce_derivative(logits, true_labels):\n",
    "            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Stability trick\n",
    "            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "            batch_size = logits.shape[0]\n",
    "            probabilities[np.arange(batch_size), true_labels] -= 1\n",
    "            gradient = probabilities / batch_size\n",
    "            \n",
    "            return gradient\n",
    "        super().__init__(sce,sce_derivative)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CategoricalCrossEntropyLoss(Loss):\n",
    "    def __init__(self):\n",
    "        def cel(y_predSM,y_true):\n",
    "            # direct correspondance of values, element wise multi\n",
    "            return -np.mean(np.sum(y_true * np.log(y_predSM+ 1e-9), axis=1))\n",
    "        def cel_derivative(y_predSM,y_true):\n",
    "            return y_predSM-y_true\n",
    "        # der wrt SM = der wrt logit(z)\n",
    "        super().__init__(cel,cel_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model:\n",
    "    def train(self,images,labels,layers,hyperpara):\n",
    "        \n",
    "        for key, value in hyperpara.items():\n",
    "            setattr(self, key, value)\n",
    "        self.loss_fn=CategoricalCrossEntropyLoss()\n",
    "        self.X=images\n",
    "        self.y=labels\n",
    "        self.layers=layers\n",
    "        self.softmax=Softmax(self.neurons[-1],self.softmax_neurons)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            num_samples = self.X.shape[0]\n",
    "            # index=np.arange(num_samples)\n",
    "            # np.random.shuffle(index)\n",
    "            # self.X=self.X[index]\n",
    "            # self.y=self.y[index]\n",
    "            epoch_loss=0\n",
    "            for i in range(0, num_samples, self.batch_size):\n",
    "                if(i+self.batch_size==num_samples):\n",
    "                    X_batch = self.X[i:num_samples]\n",
    "                    y_batch = self.y[i:num_samples]\n",
    "                else:\n",
    "                    X_batch = self.X[i:i + self.batch_size]\n",
    "                    y_batch = self.y[i:i + self.batch_size]\n",
    "                \n",
    "                ActiPredicted=self.forward(X_batch)\n",
    "                \n",
    "                pred_y=self.softmax.forward(ActiPredicted)\n",
    "                \n",
    "                loss=self.loss_fn.forward(pred_y,y_batch)\n",
    "                epoch_loss += loss\n",
    "                loss_gradient=self.loss_fn.backward(pred_y,y_batch)\n",
    "                # print(f\"Epoch {epoch + 1}, Batch {i // self.batch_size + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "                # Backward pass\n",
    "                loss_gradient=self.softmax.backward(loss_gradient,self.learning_rate,self.momentum_coeff)\n",
    "                self.backward(loss_gradient)\n",
    "            print(f\"Epoch {epoch + 1}, Average Loss: {epoch_loss / (num_samples / self.batch_size):.4f}\")\n",
    "                \n",
    "                \n",
    "                    \n",
    "                \n",
    "    def forward(self,X):\n",
    "        for layer in self.layers:\n",
    "            # here layer is obj to class not class*\n",
    "            X=layer.forward(X)\n",
    "        return X\n",
    "        \n",
    "\n",
    "    def backward(self, loss_gradient):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_gradient = layer.backward(loss_gradient, self.learning_rate,self.momentum_coeff)\n",
    "    def predict(self, X):\n",
    "        logits = self.forward(X)\n",
    "        probabilities = self.softmax.forward(logits)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        true_labels = np.argmax(y, axis=1)\n",
    "        accuracy = np.mean(predictions == true_labels)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return accuracy\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 3.1205\n",
      "Epoch 2, Average Loss: 2.8277\n",
      "Epoch 3, Average Loss: 2.6148\n",
      "Epoch 4, Average Loss: 2.4819\n",
      "Epoch 5, Average Loss: 2.3977\n",
      "Epoch 6, Average Loss: 2.3411\n",
      "Epoch 7, Average Loss: 2.2985\n",
      "Epoch 8, Average Loss: 2.2662\n",
      "Epoch 9, Average Loss: 2.2416\n",
      "Epoch 10, Average Loss: 2.2221\n",
      "Epoch 11, Average Loss: 2.2059\n",
      "Epoch 12, Average Loss: 2.1919\n",
      "Epoch 13, Average Loss: 2.1802\n",
      "Epoch 14, Average Loss: 2.1695\n",
      "Epoch 15, Average Loss: 2.1601\n",
      "Epoch 16, Average Loss: 2.1516\n",
      "Epoch 17, Average Loss: 2.1445\n",
      "Epoch 18, Average Loss: 2.1381\n",
      "Epoch 19, Average Loss: 2.1323\n",
      "Epoch 20, Average Loss: 2.1274\n",
      "Epoch 21, Average Loss: 2.1231\n",
      "Epoch 22, Average Loss: 2.1192\n",
      "Epoch 23, Average Loss: 2.1158\n",
      "Epoch 24, Average Loss: 2.1128\n",
      "Epoch 25, Average Loss: 2.1101\n",
      "Epoch 26, Average Loss: 2.1076\n",
      "Epoch 27, Average Loss: 2.1055\n",
      "Epoch 28, Average Loss: 2.1035\n",
      "Epoch 29, Average Loss: 2.1018\n",
      "Epoch 30, Average Loss: 2.1003\n",
      "Epoch 31, Average Loss: 2.0988\n",
      "Epoch 32, Average Loss: 2.0974\n",
      "Epoch 33, Average Loss: 2.0962\n",
      "Epoch 34, Average Loss: 2.0951\n",
      "Epoch 35, Average Loss: 2.0942\n",
      "Epoch 36, Average Loss: 2.0933\n",
      "Epoch 37, Average Loss: 2.0926\n",
      "Epoch 38, Average Loss: 2.0919\n",
      "Epoch 39, Average Loss: 2.0913\n",
      "Epoch 40, Average Loss: 2.0908\n",
      "Epoch 41, Average Loss: 2.0902\n",
      "Epoch 42, Average Loss: 2.0895\n",
      "Epoch 43, Average Loss: 2.0889\n",
      "Epoch 44, Average Loss: 2.0884\n",
      "Epoch 45, Average Loss: 2.0878\n",
      "Epoch 46, Average Loss: 2.0873\n",
      "Epoch 47, Average Loss: 2.0868\n",
      "Epoch 48, Average Loss: 2.0864\n",
      "Epoch 49, Average Loss: 2.0861\n",
      "Epoch 50, Average Loss: 2.0857\n",
      "Epoch 51, Average Loss: 2.0855\n",
      "Epoch 52, Average Loss: 2.0852\n",
      "Epoch 53, Average Loss: 2.0850\n",
      "Epoch 54, Average Loss: 2.0848\n",
      "Epoch 55, Average Loss: 2.0845\n",
      "Epoch 56, Average Loss: 2.0843\n",
      "Epoch 57, Average Loss: 2.0842\n",
      "Epoch 58, Average Loss: 2.0841\n",
      "Epoch 59, Average Loss: 2.0840\n",
      "Epoch 60, Average Loss: 2.0838\n",
      "Epoch 61, Average Loss: 2.0837\n",
      "Epoch 62, Average Loss: 2.0836\n",
      "Epoch 63, Average Loss: 2.0835\n",
      "Epoch 64, Average Loss: 2.0834\n",
      "Epoch 65, Average Loss: 2.0833\n",
      "Epoch 66, Average Loss: 2.0833\n",
      "Epoch 67, Average Loss: 2.0832\n",
      "Epoch 68, Average Loss: 2.0831\n",
      "Epoch 69, Average Loss: 2.0831\n",
      "Epoch 70, Average Loss: 2.0830\n",
      "Epoch 71, Average Loss: 2.0829\n",
      "Epoch 72, Average Loss: 2.0828\n",
      "Epoch 73, Average Loss: 2.0827\n",
      "Epoch 74, Average Loss: 2.0827\n",
      "Epoch 75, Average Loss: 2.0827\n",
      "Epoch 76, Average Loss: 2.0826\n",
      "Epoch 77, Average Loss: 2.0826\n",
      "Epoch 78, Average Loss: 2.0825\n",
      "Epoch 79, Average Loss: 2.0825\n",
      "Epoch 80, Average Loss: 2.0825\n",
      "Epoch 81, Average Loss: 2.0824\n",
      "Epoch 82, Average Loss: 2.0824\n",
      "Epoch 83, Average Loss: 2.0823\n",
      "Epoch 84, Average Loss: 2.0823\n",
      "Epoch 85, Average Loss: 2.0823\n",
      "Epoch 86, Average Loss: 2.0822\n",
      "Epoch 87, Average Loss: 2.0822\n",
      "Epoch 88, Average Loss: 2.0821\n",
      "Epoch 89, Average Loss: 2.0821\n",
      "Epoch 90, Average Loss: 2.0820\n",
      "Epoch 91, Average Loss: 2.0819\n",
      "Epoch 92, Average Loss: 2.0819\n",
      "Epoch 93, Average Loss: 2.0819\n",
      "Epoch 94, Average Loss: 2.0818\n",
      "Epoch 95, Average Loss: 2.0818\n",
      "Epoch 96, Average Loss: 2.0817\n",
      "Epoch 97, Average Loss: 2.0817\n",
      "Epoch 98, Average Loss: 2.0816\n",
      "Epoch 99, Average Loss: 2.0816\n",
      "Epoch 100, Average Loss: 2.0815\n",
      "Epoch 101, Average Loss: 2.0815\n",
      "Epoch 102, Average Loss: 2.0814\n",
      "Epoch 103, Average Loss: 2.0813\n",
      "Epoch 104, Average Loss: 2.0813\n",
      "Epoch 105, Average Loss: 2.0812\n",
      "Epoch 106, Average Loss: 2.0811\n",
      "Epoch 107, Average Loss: 2.0811\n",
      "Epoch 108, Average Loss: 2.0810\n",
      "Epoch 109, Average Loss: 2.0810\n",
      "Epoch 110, Average Loss: 2.0809\n",
      "Epoch 111, Average Loss: 2.0808\n",
      "Epoch 112, Average Loss: 2.0808\n",
      "Epoch 113, Average Loss: 2.0807\n",
      "Epoch 114, Average Loss: 2.0807\n",
      "Epoch 115, Average Loss: 2.0806\n",
      "Epoch 116, Average Loss: 2.0806\n",
      "Epoch 117, Average Loss: 2.0805\n",
      "Epoch 118, Average Loss: 2.0805\n",
      "Epoch 119, Average Loss: 2.0804\n",
      "Epoch 120, Average Loss: 2.0804\n",
      "Epoch 121, Average Loss: 2.0803\n",
      "Epoch 122, Average Loss: 2.0803\n",
      "Epoch 123, Average Loss: 2.0802\n",
      "Epoch 124, Average Loss: 2.0802\n",
      "Epoch 125, Average Loss: 2.0802\n",
      "Epoch 126, Average Loss: 2.0801\n",
      "Epoch 127, Average Loss: 2.0801\n",
      "Epoch 128, Average Loss: 2.0801\n",
      "Epoch 129, Average Loss: 2.0801\n",
      "Epoch 130, Average Loss: 2.0800\n",
      "Epoch 131, Average Loss: 2.0800\n",
      "Epoch 132, Average Loss: 2.0800\n",
      "Epoch 133, Average Loss: 2.0800\n",
      "Epoch 134, Average Loss: 2.0799\n",
      "Epoch 135, Average Loss: 2.0799\n",
      "Epoch 136, Average Loss: 2.0799\n",
      "Epoch 137, Average Loss: 2.0798\n",
      "Epoch 138, Average Loss: 2.0798\n",
      "Epoch 139, Average Loss: 2.0798\n",
      "Epoch 140, Average Loss: 2.0797\n",
      "Epoch 141, Average Loss: 2.0797\n",
      "Epoch 142, Average Loss: 2.0797\n",
      "Epoch 143, Average Loss: 2.0797\n",
      "Epoch 144, Average Loss: 2.0796\n",
      "Epoch 145, Average Loss: 2.0796\n",
      "Epoch 146, Average Loss: 2.0796\n",
      "Epoch 147, Average Loss: 2.0795\n",
      "Epoch 148, Average Loss: 2.0795\n",
      "Epoch 149, Average Loss: 2.0795\n",
      "Epoch 150, Average Loss: 2.0794\n"
     ]
    }
   ],
   "source": [
    "hyperparameters={\n",
    "    \"hiddenLayers\":3,\n",
    "    \"learning_rate\":0.00001,\n",
    "    \"batch_size\":32,\n",
    "    \"epochs\":150,\n",
    "    \"ActivationFns\":None,\n",
    "    \"neurons\":None,\n",
    "    \"softmax_neurons\":labels.shape[1],\n",
    "    \"momentum_coeff\":0.9\n",
    "    # v=coeff*v-learnrate*gradient\n",
    "    # w=w+v\n",
    "    \n",
    "}\n",
    "\n",
    "# change\n",
    "hyperparameters[\"neurons\"]=[32,32,16]\n",
    "hyperparameters[\"ActivationFns\"]=[Relu(),Relu(),Relu()]\n",
    "layers=[]\n",
    "inputsize=images.shape[1]\n",
    "for i in range(hyperparameters[\"hiddenLayers\"]):\n",
    "    outputsize=hyperparameters[\"neurons\"][i]\n",
    "    layers.extend([Dense(inputsize,outputsize),hyperparameters[\"ActivationFns\"][i]])\n",
    "    inputsize=outputsize\n",
    "\n",
    "\n",
    "model=Model()\n",
    "model.train(images,labels,layers,hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 17.60%\n",
      "accuary:0.1759656652360515\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuary:{model.evaluate(test_images,test_labels)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skindisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
