{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class driveFile:\n",
    "  def __init__(self):\n",
    "    self.path=\"\"\n",
    "  def dataload(self,folder_path):\n",
    "    img=[]\n",
    "    label=[]\n",
    "    import os\n",
    "    self.path1=os.path.join(self.path,folder_path)\n",
    "    \n",
    "    if os.path.exists(self.path1):\n",
    "      for i in os.listdir(self.path1):\n",
    "        self.dis_path=os.path.join(self.path1,i)\n",
    "        # /content/drive/MyDrive/train_set/BA- cellulitis\n",
    "        if os.path.exists(self.dis_path) and '.DS_Store' not in self.dis_path:\n",
    "          for j in os.listdir(self.dis_path):\n",
    "           if j.endswith(('.jpg', '.png', '.jpeg')):\n",
    "              self.img_path = os.path.join(self.dis_path, j)\n",
    "              loaded_img= self.imgload(self.img_path)\n",
    "              if loaded_img is not None:\n",
    "                img.append(loaded_img)\n",
    "                label.append(i)\n",
    "              else:\n",
    "                 print(\"Skipping non-image file:\", j)\n",
    "          else:\n",
    "              print(\"Skipping invalid path or .DS_Store:\", self.dis_path)\n",
    "              continue\n",
    "      return np.array(img), np.array(label)\n",
    "    else:\n",
    "      return \"patherror\",\"patherror\"\n",
    "\n",
    "\n",
    "\n",
    "  def imgload(self,imgpath):\n",
    "    img=cv2.imread(imgpath)\n",
    "    if img is None:\n",
    "      print(f\"Failed to load image at {imgpath}\")\n",
    "      return None\n",
    "    img=cv2.resize(img,(20,20))\n",
    "    # v imp, usually 224*224 but here 20*20 \"downsampling or using a smaller image resolution before flattening it.\"\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    img=img.astype('float32')/255.0\n",
    "    img_array=np.array(img)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid path or .DS_Store: train_set/FU-athlete-foot\n",
      "Skipping invalid path or .DS_Store: train_set/BA- cellulitis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drive=driveFile()\n",
    "images,labels=drive.dataload('train_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=np.array([img.flatten() for img in images])\n",
    "labels=np.array(labels)\n",
    "j=0\n",
    "label_codes=dict()\n",
    "for i in np.unique(labels):\n",
    "    label_codes[i]=j\n",
    "    j+=1\n",
    "label_codes\n",
    "labels=np.array([label_codes[i] for i in labels])\n",
    "del i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input=None\n",
    "        self.output=None\n",
    "    def forward(self,input):\n",
    "        # to be overridden \n",
    "        pass \n",
    "    def backward(self,output_gradient,learning_rate):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        self.weights=np.random.randn(output_size,input_size)*0.01\n",
    "        self.bias=np.random.randn(output_size,1)\n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        return np.dot(self.weights,self.input)+self.bias\n",
    "    def backward(self,output_gradient,learning_rate):\n",
    "        w_gradient = np.dot(output_gradient, self.input.T)  # (output_size, input_size)\n",
    "        b_gradient = np.sum(output_gradient, axis=1, keepdims=True)  # (output_size, 1)\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)  # (input_size, batch_size)\n",
    "        self.weights-=learning_rate * w_gradient\n",
    "        self.bias-=learning_rate * b_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self,activation,activatn_derivative):\n",
    "        #activation is a variable pointing to a activation method,for ex sigmoid\n",
    "        self.activation=activation\n",
    "        self.activatn_derivative=activatn_derivative\n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        return self.activation(self.input)\n",
    "    def backward(self,output_gradient,learning_rate):\n",
    "        return np.multiply(output_gradient,self.activatn_derivative(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1/(1+np.exp(-x))\n",
    "        def sigmoid_derivative(x):\n",
    "            s=sigmoid(x)\n",
    "            return s*(1-s)\n",
    "        super().__init__(sigmoid,sigmoid_derivative)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return (exp(x)-exp(-x))/(exp(x)+exp(-x))\n",
    "        def tanh_derivative(x):\n",
    "            t=tanh(x)\n",
    "            return 1-(t**2)\n",
    "        super().__init__(tanh,tanh_derivative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input):\n",
    "        return np.exp(input)/np.sum(np.exp(input))\n",
    "    def backward():\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(Layer):\n",
    "    def __init__(self,loss_fn,loss_fn_Derivative):\n",
    "        self.loss_fn=loss_fn\n",
    "        self.loss_fn_Derivative=loss_fn_Derivative\n",
    "    def forward(self,y_pred,y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return self.loss_fn(y_pred, y_true)\n",
    "    def backward(self,y_pred,y_true):\n",
    "        return self.loss_fn_Derivative(y_pred,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy(Loss):\n",
    "    def __init__(self):\n",
    "        def bce(y_pred,y_true):\n",
    "            y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)  # Prevent extreme values\n",
    "            return -np.mean((y_true*np.log(y_pred))+((1-y_true)*np.log(1-y_pred)))\n",
    "        def bce_derivative(y_pred,y_true):\n",
    "            y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)  # Avoid division by zero\n",
    "            return -(y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "        super().__init__(bce,bce_derivative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCategoricalCE(Loss):\n",
    "    def __init__(self):\n",
    "        def sce(logits,true_labels):\n",
    "            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True)) \n",
    "            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "            true_class_probs = probabilities[np.arange(len(true_labels)), true_labels]\n",
    "            log_loss = -np.log(true_class_probs + 1e-15)  # Adding a small constant for numerical stability\n",
    "            return np.mean(log_loss)\n",
    "        def sce_derivative(logits, true_labels):\n",
    "            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Stability trick\n",
    "            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "            batch_size = logits.shape[0]\n",
    "            probabilities[np.arange(batch_size), true_labels] -= 1\n",
    "            gradient = probabilities / batch_size\n",
    "            \n",
    "            return gradient\n",
    "        super().__init__(sce,sce_derivative)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Model:\\n    def __init__(self, layers, learning_rate=0.01, batch_size=32, epochs=10):\\n        self.layers = layers\\n        self.learning_rate = learning_rate\\n        self.batch_size = batch_size\\n        self.epochs = epochs\\n        self.loss_fn = SparseCategoricalCE()\\n\\n    def forward(self, X):\\n        for layer in self.layers:\\n            X = layer.forward(X)\\n        return X\\n\\n    def backward(self, loss_gradient):\\n        for layer in reversed(self.layers):\\n            loss_gradient = layer.backward(loss_gradient, self.learning_rate)\\n\\n    def train(self, X, y):\\n        for epoch in range(self.epochs):\\n            num_samples = X.shape[0]\\n            index=np.arange(num_samples)\\n            np.random.shuffle(index)\\n            X=X[index]\\n            y=y[index]\\n            for i in range(0, num_samples, self.batch_size):\\n                if(i+self.batch_size==num_samples):\\n                    X_batch = X[i:num_samples]\\n                    y_batch = y[i:num_samples]\\n                else:\\n                    X_batch = X[i:i + self.batch_size]\\n                    y_batch = y[i:i + self.batch_size]\\n\\n                # Forward pass\\n                predictions = self.forward(X_batch.T)\\n                \\n                # Compute loss\\n                loss = self.loss_fn.forward(predictions, y_batch.T)\\n                print(f\"Epoch {epoch + 1}, Batch {i // self.batch_size + 1}, Loss: {loss}\")\\n\\n                # Backward pass\\n                loss_gradient = self.loss_fn.backward(predictions, y_batch.T)\\n                self.backward(loss_gradient)\\n\\n    def predict(self, X):\\n        predictions = self.forward(X.T)\\n        return (predictions >= 0.5).astype(int).flatten()\\n\\n    def evaluate(self, X, y):\\n        predictions = self.predict(X)\\n        accuracy = np.mean(predictions == y)\\n        print(f\"Accuracy: {accuracy * 100:.2f}%\")\\n        return accuracy\\n\\n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class Model:\n",
    "    def __init__(self, layers, learning_rate=0.01, batch_size=32, epochs=10):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.loss_fn = SparseCategoricalCE()\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, loss_gradient):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_gradient = layer.backward(loss_gradient, self.learning_rate)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            num_samples = X.shape[0]\n",
    "            index=np.arange(num_samples)\n",
    "            np.random.shuffle(index)\n",
    "            X=X[index]\n",
    "            y=y[index]\n",
    "            for i in range(0, num_samples, self.batch_size):\n",
    "                if(i+self.batch_size==num_samples):\n",
    "                    X_batch = X[i:num_samples]\n",
    "                    y_batch = y[i:num_samples]\n",
    "                else:\n",
    "                    X_batch = X[i:i + self.batch_size]\n",
    "                    y_batch = y[i:i + self.batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.forward(X_batch.T)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.loss_fn.forward(predictions, y_batch.T)\n",
    "                print(f\"Epoch {epoch + 1}, Batch {i // self.batch_size + 1}, Loss: {loss}\")\n",
    "\n",
    "                # Backward pass\n",
    "                loss_gradient = self.loss_fn.backward(predictions, y_batch.T)\n",
    "                self.backward(loss_gradient)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.forward(X.T)\n",
    "        return (predictions >= 0.5).astype(int).flatten()\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return accuracy\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model:\n",
    "    def train(self,images,labels,layers,hyperpara):\n",
    "        \n",
    "        for key, value in hyperpara.items():\n",
    "            setattr(self, key, value)\n",
    "        self.loss_fn=SparseCategoricalCE()\n",
    "        self.X=images\n",
    "        self.y=labels\n",
    "        self.layers=layers\n",
    "        self.softmax=Softmax()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            num_samples = self.X.shape[0]\n",
    "            index=np.arange(num_samples)\n",
    "            np.random.shuffle(index)\n",
    "            self.X=self.X[index]\n",
    "            self.y=self.y[index]\n",
    "            for i in range(0, num_samples, self.batch_size):\n",
    "                if(i+self.batch_size==num_samples):\n",
    "                    X_batch = self.X[i:num_samples]\n",
    "                    y_batch = self.y[i:num_samples]\n",
    "                else:\n",
    "                    X_batch = self.X[i:i + self.batch_size]\n",
    "                    y_batch = self.y[i:i + self.batch_size]\n",
    "                ActiPredicted=self.forward(X_batch)\n",
    "                pred_y=self.softmax.forward(ActiPredicted)\n",
    "                \n",
    "                loss=self.loss_fn.forward(pred_y,y_batch)\n",
    "                dloss=self.loss_fn.backward(pred_y,y_batch)\n",
    "                dsoft=self.softmax.forward(ActiPredicted)\n",
    "                \n",
    "                print(f\"Epoch {epoch + 1}, Batch {i // self.batch_size + 1}, Loss: {loss}\")\n",
    "\n",
    "                # Backward pass\n",
    "                \n",
    "                self.backward(loss_gradient)\n",
    "                \n",
    "                \n",
    "                    \n",
    "                \n",
    "    def forward(self,X):\n",
    "        for layer in self.layers:\n",
    "            # here layer is obj to class not class*\n",
    "            X=layer.forward(X)\n",
    "        return X\n",
    "        \n",
    "\n",
    "    def backward(self, loss_gradient):\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_gradient = layer.backward(loss_gradient, self.learning_rate)\n",
    "    def predict(self, X):\n",
    "        predictions = self.forward(X.T)\n",
    "        return (predictions >= 0.5).astype(int).flatten()\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return accuracy\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,400) and (25,400) not aligned: 400 (dim 1) != 25 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m     inputsize\u001b[38;5;241m=\u001b[39moutputsize\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m=\u001b[39mModel()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[104], line 25\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, images, labels, layers, hyperpara)\u001b[0m\n\u001b[1;32m     23\u001b[0m     X_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m     24\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m---> 25\u001b[0m ActiPredicted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m pred_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax\u001b[38;5;241m.\u001b[39mforward(ActiPredicted)\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn\u001b[38;5;241m.\u001b[39mforward(pred_y,y_batch)\n",
      "Cell \u001b[0;32mIn[104], line 44\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# here layer is obj to class not class*\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m         X\u001b[38;5;241m=\u001b[39m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "Cell \u001b[0;32mIn[95], line 7\u001b[0m, in \u001b[0;36mDense.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,400) and (25,400) not aligned: 400 (dim 1) != 25 (dim 0)"
     ]
    }
   ],
   "source": [
    "hyperparameters={\n",
    "    \"hiddenLayers\":2,\n",
    "    \"learning_rate\":0.01,\n",
    "    \"batch_size\":25,\n",
    "    \"epochs\":15,\n",
    "    \"ActivationFns\":None,\n",
    "    \"neurons\":None\n",
    "}\n",
    "# change\n",
    "hyperparameters[\"neurons\"]=[2,1]\n",
    "hyperparameters[\"ActivationFns\"]=['Sigmoid()','Tanh()']\n",
    "layers=[]\n",
    "inputsize=images.shape[1]\n",
    "for i in range(hyperparameters[\"hiddenLayers\"]):\n",
    "    outputsize=hyperparameters[\"neurons\"][i]\n",
    "    layers.extend([Dense(inputsize,outputsize),hyperparameters[\"ActivationFns\"][i]])\n",
    "    inputsize=outputsize\n",
    "\n",
    "\n",
    "model=Model()\n",
    "model.train(images,labels,layers,hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skindisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
